{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MountainCar Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Environment information\n",
    "ref: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py#L16\n",
    "\n",
    "Observation Space\n",
    "    The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
    "    | Num | Observation                          | Min  | Max | Unit         |\n",
    "    |-----|--------------------------------------|------|-----|--------------|\n",
    "    | 0   | position of the car along the x-axis | -Inf | Inf | position (m) |\n",
    "    | 1   | velocity of the car                  | -Inf | Inf | position (m) |\n",
    "Action Space\n",
    "    There are 3 discrete deterministic actions:\n",
    "    | Num | Observation             | Value | Unit         |\n",
    "    |-----|-------------------------|-------|--------------|\n",
    "    | 0   | Accelerate to the left  | Inf   | position (m) |\n",
    "    | 1   | Don't accelerate        | Inf   | position (m) |\n",
    "    | 2   | Accelerate to the right | Inf   | position (m) |\n",
    "Reward:\n",
    "    The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is\n",
    "    penalised with a reward of -1 for each timestep.\n",
    "Starting State\n",
    "    The position of the car is assigned a uniform random value in *[-0.6 , -0.4]*.\n",
    "    The starting velocity of the car is always assigned to 0.\n",
    "Episode End\n",
    "    The episode ends if either of the following happens:\n",
    "    1. Termination: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "    2. Truncation: The length of the episode is 200.\n",
    "\"\"\"\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "# env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample environment image\n",
    "\n",
    "<img width=300 src=\"mountain_car.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space:  Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "action_space:  Discrete(3)\n",
      "sample obs:  (-0.46812093, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_space: \", env.observation_space)\n",
    "print(\"action_space: \", env.action_space)\n",
    "state_Xposition, state_velocity = env.reset()\n",
    "print(\"sample obs: \", (state_Xposition, state_velocity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample step:  (array([-4.6853510e-01, -4.1416552e-04], dtype=float32), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# sample step\n",
    "new_state, reward, done, info = env.step(1)\n",
    "print(\"sample step: \", (new_state, reward, done, info))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "class ExperienceReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Store samples of transitions based on applying epsilon-greedy with Q-Network and replay them to train the network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Randomly sample a batch of transitions\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transition(state=1, action=2, reward=3, next_state=4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample transition\n",
    "state, action, reward, next_state = [1, 2, 3, 4]\n",
    "Transition(state, action, reward, next_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(VanillaNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, memory_size=10000, batch_size=128, gamma=0.99, epsilon_start=0.9, epsilon_end=0.05, epsilon_decay=1000, tau=0.005, learning_rate=1e-4):\n",
    "        self.env = env  # the gym environment\n",
    "        self.memory_size = memory_size  # the size of the replay buffer\n",
    "        self.batch_size = batch_size  # number of transitions sampled from the replay buffer\n",
    "        self.gamma = gamma  # the discount factor as mentioned in the previous section\n",
    "        self.epsilon_start = epsilon_start  # the starting value of epsilon\n",
    "        self.epsilon_end = epsilon_end  # the final value of epsilon\n",
    "        self.epsilon_decay = epsilon_decay  # controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "        self.tau = tau  # the update rate of the target network\n",
    "        self.learning_rate = learning_rate  # the learning rate of the optimizer\n",
    "        \n",
    "        self.steps_done = 0  # the number of steps taken in the environment\n",
    "        \n",
    "        # get number of actions and observations\n",
    "        self._n_actions = self.env.action_space.n\n",
    "        self._n_observations = len(env.reset())\n",
    "        \n",
    "        # setup NN models\n",
    "        self.policy_net = VanillaNN(self._n_observations, self._n_actions).to(device)\n",
    "        self.target_net = VanillaNN(self._n_observations, self._n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        # setup optimizer and loss function\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.learning_rate, amsgrad=True)\n",
    "        self.criterion = nn.MSELoss()  # ref: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
    "        # self.criterion = nn.SmoothL1Loss()  # ref: https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html\n",
    "        \n",
    "        # setup Experience memory\n",
    "        self.memory = ExperienceReplayMemory(self.memory_size)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Chooses an action using an epsilon-greedy policy (with decay rate).\n",
    "        \"\"\"\n",
    "        epsilon_threshold = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * math.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        if np.random.uniform(0, 1) > epsilon_threshold:\n",
    "            with torch.no_grad():\n",
    "                # return action with highest Q value\n",
    "                state_Qs = self.policy_net(state)  # get Q values for all actions\n",
    "                action_max_Q = state_Qs.max(1)[1]  # get the action with the highest Q value\n",
    "                return action_max_Q.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def train(self, num_episodes=1000, max_steps=200, target_soft_update=False, target_update_freq=1000):\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                # choose and take action using epsilon-greedy policy\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action.item())\n",
    "\n",
    "                # store the transition (state, action, reward, next_state) in memory\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "                self.memory.push(state, action, reward, next_state)\n",
    "                \n",
    "                # move to the next state\n",
    "                state = next_state\n",
    "                \n",
    "                # perform one step of the optimization on the policy network\n",
    "                self.optimize_model()\n",
    "                \n",
    "                # update target network\n",
    "                if target_soft_update:\n",
    "                    # Soft update of the target network weights (θ′ ← τθ + (1−τ)θ′)\n",
    "                    target_net_state_dict = self.target_net.state_dict()\n",
    "                    policy_net_state_dict = self.policy_net.state_dict()\n",
    "                    for key in policy_net_state_dict:\n",
    "                        target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
    "                    self.target_net.load_state_dict(target_net_state_dict)\n",
    "                else:\n",
    "                    # Hard update of the target network weights (θ′ ← θ)\n",
    "                    if self.steps_done % target_update_freq == 0:\n",
    "                        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "                \n",
    "                # update step counter\n",
    "                self.steps_done += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # get a batch of transitions from the replay buffer\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # compute a mask of non-final states and concatenate the batch elements (final state is the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # compute Q(s_t, a) using the policy_net model and select the columns of actions taken for each batch state\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # compute V(s_{t+1}) for all next states \n",
    "        # where the expected values of actions are computed based on the target_net model and selected based on the action with max state value\n",
    "        # note: V(s_{t+1}) = 0 if the state was the final state of the simulation (i.e. non_final_mask is False)\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "        # compute the expected (target) Q values\n",
    "        expected_state_action_values = reward_batch + (self.gamma * next_state_values)\n",
    "\n",
    "        # compute MSE loss\n",
    "        loss = self.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # optimize the model with backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # in-place gradient clipping (to avoid exploding gradients)\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def generate_sample_episode(self, max_steps=200, display=False):\n",
    "        episode = list()\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        for step in tqdm(range(max_steps)):\n",
    "            # choose and take action using epsilon-greedy policy\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action.item())\n",
    "            if display:\n",
    "                self.env.render()\n",
    "\n",
    "            # store the transition (state, action, reward, next_state) in memory\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "            episode.append([state, action, reward, next_state])\n",
    "            \n",
    "            # move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:52<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "dqn_agent = DQN(env)\n",
    "dqn_agent.train(num_episodes=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 199/200 [00:01<00:00, 118.78it/s]\n"
     ]
    }
   ],
   "source": [
    "episode = dqn_agent.generate_sample_episode(display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
