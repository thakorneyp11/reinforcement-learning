{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Environment Information\n",
    "ref: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L17\n",
    "\n",
    "Action Space\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "Observation Space\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "Rewards\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
    "    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "Episode End\n",
    "    The episode ends if any one of the following occurs:\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\"\"\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample environment image\n",
    "\n",
    "<img width=300 src=\"cartpole.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action_space:  Discrete(2)\n",
      "sample obs:  [ 0.01738542 -0.02711608 -0.03005168 -0.00942616]\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_space: \", env.observation_space)\n",
    "print(\"action_space: \", env.action_space)\n",
    "state = env.reset()\n",
    "print(\"sample obs: \", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample step:  (array([ 0.0168431 ,  0.16842367, -0.0302402 , -0.31143722], dtype=float32), 1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# sample step\n",
    "new_state, reward, terminate, info = env.step(env.action_space.sample())\n",
    "print(\"sample step: \", (new_state, reward, terminate, info))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Actor-Critic (base class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNN(nn.Module):\n",
    "    \"\"\" Actor Network represents the agent's policy\n",
    "    forward output: probability distribution over actions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=-1)  # dim need to be -1 to prevent NaN results\n",
    "        return x\n",
    "    \n",
    "class CriticNN(nn.Module):\n",
    "    \"\"\" Critic Network represents the agent's value function, typically represented as a function approximator \n",
    "    forward output: expected cumulative reward from a given state (or state-action pair)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, n_actions, is_value=False):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        if is_value:\n",
    "            self.layer3 = nn.Linear(128, 1)\n",
    "        else:\n",
    "            self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor net (probs):  tensor([0.4941, 0.5059], grad_fn=<SoftmaxBackward0>)\n",
      "critic net (value):  tensor([-0.0517, -0.0616], grad_fn=<AddBackward0>)\n",
      "critic net (value):  tensor([-0.0672], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample_actor = ActorNN(n_observations=env.observation_space.shape[0], n_actions=env.action_space.n)\n",
    "pred = sample_actor(torch.tensor(state).float())\n",
    "print(\"actor net (probs): \", pred)\n",
    "\n",
    "sample_critic = CriticNN(n_observations=env.observation_space.shape[0], n_actions=env.action_space.n)\n",
    "pred = sample_critic(torch.tensor(state).float())\n",
    "print(\"critic net (value): \", pred)\n",
    "\n",
    "sample_critic = CriticNN(n_observations=env.observation_space.shape[0], n_actions=env.action_space.n, is_value=True)\n",
    "pred = sample_critic(torch.tensor(state).float())\n",
    "print(\"critic net (value): \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, env, gamma=0.99, learning_rate=3e-4, critic_value=False, device=\"cpu\"):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.critic_value = critic_value\n",
    "        self.device = device\n",
    "\n",
    "        # get number of actions and observations\n",
    "        self._n_observations = self.env.observation_space.shape[0]\n",
    "        self._n_actions = self.env.action_space.n\n",
    "\n",
    "        # setup NN model\n",
    "        self.actor_net = ActorNN(self._n_observations, self._n_actions).to(self.device)\n",
    "        self.critic_net = CriticNN(self._n_observations, self._n_actions, is_value=self.critic_value).to(self.device)\n",
    "\n",
    "        # setup optimizer\n",
    "        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), lr=self.learning_rate)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_net.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        # TODO: apply epsilon-greedy for exploration\n",
    "        _state = torch.from_numpy(state).float()\n",
    "        action_probs = self.actor_net(_state)\n",
    "        m = Categorical(action_probs)\n",
    "        selected_action = m.sample()\n",
    "        log_prop = m.log_prob(selected_action)\n",
    "        return selected_action.item(), log_prop\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"loss calculation for actor and critic network\"\"\"\n",
    "        # return actor_loss, critic_loss\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, num_episodes=1000, max_steps=500, batch_size=1, log_interval=100, solve_score=450):\n",
    "        episode_rewards = list()\n",
    "        experience_buffer = list()\n",
    "        running_rewards = deque(maxlen=100)  # track recent last return to identify if the environment is solved\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                action, log_prop = self.choose_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                # store transitions\n",
    "                experience_buffer.append((state, action, reward, next_state, done, log_prop))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done or len(experience_buffer) >= batch_size:\n",
    "                    # update actor and critic network using the collected experiences\n",
    "                    states, actions, rewards, next_states, dones, log_props = zip(*experience_buffer)\n",
    "                    self.update(states, actions, rewards, next_states, dones, log_props)\n",
    "                    # clear the buffer\n",
    "                    experience_buffer.clear()\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            # log results\n",
    "            running_rewards.append(episode_reward)\n",
    "            running_mean = np.array(running_rewards).mean()\n",
    "            running_std_dev = np.array(running_rewards).std()\n",
    "            running_max = np.array(running_rewards).max()\n",
    "            if (episode+1) % log_interval == 0:\n",
    "                print(f\"Episode {episode+1}\\taverage reward: {running_mean:.2f}, std dev: {running_std_dev:.2f}, max: {running_max:.2f}\")\n",
    "            \n",
    "            # check for early stopping\n",
    "            if running_mean >= solve_score and len(running_rewards) >= 100:\n",
    "                print(f\"Solved! Running reward is now {running_mean:.2f}\")\n",
    "                print(f\"Episode {episode+1}\\taverage reward: {running_mean:.2f}, std dev: {running_std_dev:.2f}, max: {running_max:.2f}\")\n",
    "                break\n",
    "            \n",
    "        return episode_rewards\n",
    "\n",
    "    def update(self, states, actions, rewards, next_states, dones, log_props):\n",
    "        # preprocess transitions (change to numpy-array first to speed up the tensor conversion process)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32)\n",
    "        log_props = torch.stack(log_props)\n",
    "\n",
    "        # calculate loss on actor and critic network\n",
    "        actor_loss, critic_loss = self.compute_loss(states, actions, rewards, next_states, dones, log_props)\n",
    "\n",
    "        # perform one step of the optimization on the actor network\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # perform one step of the optimization on the critic network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    def get_trajectory(self, max_steps=500, render=False):\n",
    "        trajectory = list()\n",
    "        done = False  # incase of early loop-termination (max_steps) before the environment terminated\n",
    "        state = self.env.reset()\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            selected_action, _ = self.choose_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(selected_action)\n",
    "            trajectory.append((state, selected_action, reward))\n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return trajectory, done\n",
    "    \n",
    "    def visualize_policy(self, num_episodes=1, max_steps=500):\n",
    "        for _ in range(num_episodes):\n",
    "            _ = self.get_trajectory(max_steps=max_steps, render=True)\n",
    "            \n",
    "    def evaluate_policy(self, num_episodes=100, max_steps=500):\n",
    "        win = 0\n",
    "        for _ in range(num_episodes):\n",
    "            trajectory, _ = self.get_trajectory(max_steps=max_steps)\n",
    "            # CUSTOMIZE: for cartpole environment\n",
    "            if len(trajectory) >= 490:\n",
    "                win += 1\n",
    "        return win / num_episodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advantage Actor-Critic (A2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvantageActorCritic(ActorCritic):\n",
    "    critic_value = True  # set Critic Network to estimate the Value of a given state\n",
    "\n",
    "    def __init__(self, env, gamma=0.99, learning_rate=3e-3, device=\"cpu\"):\n",
    "        super().__init__(env, gamma, learning_rate, self.critic_value, device)\n",
    "\n",
    "    def _compute_returns(self, next_value, rewards, masks):\n",
    "        R = next_value\n",
    "        returns = list()\n",
    "        for idx in reversed(range(len(rewards))):\n",
    "            R = rewards[idx] + self.gamma * R * masks[idx]\n",
    "            returns.insert(0, R)\n",
    "        return torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    def compute_loss(self, states, actions, rewards, next_states, dones, log_props):\n",
    "        masks = 1 - dones  # prevent the target Q-value from being updated when an episode ends (done: mask=0, not done: mask=1)\n",
    "\n",
    "        # estimate the Values of current state and next state using Critic Network\n",
    "        values = self.critic_net(states).squeeze(-1)\n",
    "        batch_next_value = self.critic_net(next_states[-1])  # for the last state in the batch\n",
    "\n",
    "        # calculate the target value (expected real value at current state)\n",
    "        expected_returns = self._compute_returns(batch_next_value, rewards, masks)\n",
    "        target_values = rewards + self.gamma * expected_returns * masks\n",
    "\n",
    "        # compute MSE loss for Critic Network\n",
    "        mse_loss = nn.MSELoss()\n",
    "        critic_loss = mse_loss(values, target_values.detach())\n",
    "\n",
    "        # compute loss for Actor Network \n",
    "        # calculate advantage function\n",
    "        advantages = expected_returns - values.detach()\n",
    "        actor_loss = -(log_props * advantages.detach()).mean()\n",
    "\n",
    "        return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\taverage reward: 63.53, std dev: 52.98, max: 200.00\n",
      "Episode 200\taverage reward: 174.20, std dev: 43.24, max: 200.00\n",
      "Solved! Running reward is now 200.00\n",
      "Episode 294\taverage reward: 200.00, std dev: 0.00, max: 200.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "a2c_agent = AdvantageActorCritic(env)\n",
    "episode_rewards = a2c_agent.train(num_episodes=int(1e3), max_steps=200, batch_size=12, log_interval=100, solve_score=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done status:  True\n",
      "step taken (maximum 500):  500\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent in 1 episode\n",
    "trajectory, done = a2c_agent.get_trajectory(max_steps=501)\n",
    "print(\"done status: \", done)\n",
    "print(\"step taken (maximum 500): \", len(trajectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 100.00% from 100 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "test_episodes = 100\n",
    "win_rate = a2c_agent.evaluate_policy(num_episodes=test_episodes)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the trained agent on a separate window\n",
    "a2c_agent.visualize_policy(num_episodes=10, max_steps=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TD Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDActorCritic(ActorCritic):\n",
    "    critic_value = True  # set Critic Network to estimate the Value of a given state\n",
    "\n",
    "    def __init__(self, env, gamma=0.99, learning_rate=3e-3, device=\"cpu\"):\n",
    "        super().__init__(env, gamma, learning_rate, self.critic_value, device)\n",
    "\n",
    "    def compute_loss(self, states, actions, rewards, next_states, dones, log_props):\n",
    "        masks = 1 - dones  # prevent the target Q-value from being updated when an episode ends (done: mask=0, not done: mask=1)\n",
    "\n",
    "        # estimate the Values of current state and next state using Critic Network\n",
    "        values = self.critic_net(states).squeeze(-1)\n",
    "        batch_next_value = self.critic_net(next_states[-1])  # for the last state in the batch\n",
    "\n",
    "        # calculate the target value (expected real value at current state)\n",
    "        expected_returns = self._compute_returns(batch_next_value, rewards, masks)\n",
    "        target_values = rewards + self.gamma * expected_returns * masks\n",
    "\n",
    "        # compute MSE loss for Critic Network\n",
    "        mse_loss = nn.MSELoss()\n",
    "        critic_loss = mse_loss(values, target_values.detach())\n",
    "\n",
    "        # compute loss for Actor Network\n",
    "        # calculate TD error\n",
    "        td_error = target_values - values.detach()\n",
    "        actor_loss = -(log_props * td_error.detach()).mean()\n",
    "\n",
    "        return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\taverage reward: 62.69, std dev: 66.82, max: 200.00\n",
      "Episode 200\taverage reward: 136.11, std dev: 75.04, max: 200.00\n",
      "Episode 300\taverage reward: 168.15, std dev: 44.55, max: 200.00\n",
      "Solved! Running reward is now 200.00\n",
      "Episode 349\taverage reward: 200.00, std dev: 0.00, max: 200.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "td_ac_agent = TDActorCritic(env)\n",
    "episode_rewards = td_ac_agent.train(num_episodes=int(1e3), max_steps=200, batch_size=12, log_interval=100, solve_score=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done status:  True\n",
      "step taken (maximum 500):  500\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent in 1 episode\n",
    "trajectory, done = td_ac_agent.get_trajectory(max_steps=501)\n",
    "print(\"done status: \", done)\n",
    "print(\"step taken (maximum 500): \", len(trajectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 100.00% from 100 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "test_episodes = 100\n",
    "win_rate = td_ac_agent.evaluate_policy(num_episodes=test_episodes)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the trained agent on a separate window\n",
    "td_ac_agent.visualize_policy(num_episodes=10, max_steps=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Q Actor-Critic (not success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNN2(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(ActorNN2, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 64)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.layer2 = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(self.dropout(x))\n",
    "        x = F.softmax(self.layer2(x), dim=-1)  # dim need to be -1 to prevent NaN results\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QActorCritic(ActorCritic):\n",
    "    critic_value = False  # set Critic Network to estimate Q-Values\n",
    "    \n",
    "    def __init__(self, env, gamma=0.99, learning_rate=3e-4, epsilon=0.1, device=\"cpu\"):\n",
    "        super().__init__(env, gamma, learning_rate, self.critic_value, device)\n",
    "        self.actor_net = ActorNN2(self._n_observations, self._n_actions).to(self.device)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # apply epsilon-greedy policy\n",
    "    def choose_action(self, state):\n",
    "        _state = torch.from_numpy(state).float()\n",
    "        action_probs = self.actor_net(_state)\n",
    "        if random.random() < self.epsilon:\n",
    "            # Random action\n",
    "            selected_action = np.random.choice(self._n_actions)\n",
    "        else:\n",
    "            m = Categorical(action_probs)\n",
    "            selected_action = m.sample().item()\n",
    "        log_prop = torch.log(action_probs[selected_action])\n",
    "        return selected_action, log_prop\n",
    "\n",
    "    def compute_loss(self, states, actions, rewards, next_states, dones, log_props):\n",
    "        masks = 1 - dones  # prevent the target Q-value from being updated when an episode ends (done: mask=0, not done: mask=1)\n",
    "\n",
    "        # estimate the Q-values of current state and next state using Critic Network\n",
    "        q_values = self.critic_net(states)\n",
    "        batch_next_q_values = self.critic_net(next_states[-1])\n",
    "        \n",
    "        # calculate the target Q-value (expected real Q-value at current state)\n",
    "        selected_q_values = q_values.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # calculate the target value (expected real value at current state)\n",
    "        batch_max_q_value = batch_next_q_values.max(dim=-1).values\n",
    "        target_q_values = rewards + self.gamma * batch_max_q_value * masks\n",
    "\n",
    "        # compute MSE loss for Critic Network\n",
    "        mse_loss = nn.MSELoss()\n",
    "        critic_loss = mse_loss(selected_q_values, target_q_values.detach())\n",
    "\n",
    "        # compute loss for Actor Network\n",
    "        # calculate advantage function\n",
    "        qadvantages = target_q_values - selected_q_values.detach()\n",
    "        actor_loss = -(log_props * qadvantages.detach()).mean()\n",
    "\n",
    "        return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250\taverage reward: 23.06, std dev: 11.73, max: 59.00\n",
      "Episode 500\taverage reward: 20.50, std dev: 10.75, max: 67.00\n",
      "Episode 750\taverage reward: 20.03, std dev: 8.41, max: 64.00\n",
      "Episode 1000\taverage reward: 20.99, std dev: 10.93, max: 62.00\n",
      "Episode 1250\taverage reward: 20.57, std dev: 12.03, max: 77.00\n",
      "Episode 1500\taverage reward: 20.40, std dev: 10.16, max: 62.00\n",
      "Episode 1750\taverage reward: 20.94, std dev: 10.54, max: 57.00\n",
      "Episode 2000\taverage reward: 22.29, std dev: 10.62, max: 77.00\n",
      "Episode 2250\taverage reward: 22.62, std dev: 12.30, max: 66.00\n",
      "Episode 2500\taverage reward: 21.78, std dev: 10.15, max: 58.00\n",
      "Episode 2750\taverage reward: 19.85, std dev: 8.87, max: 47.00\n",
      "Episode 3000\taverage reward: 21.18, std dev: 10.35, max: 75.00\n",
      "Episode 3250\taverage reward: 19.55, std dev: 10.44, max: 77.00\n",
      "Episode 3500\taverage reward: 19.92, std dev: 10.19, max: 72.00\n",
      "Episode 3750\taverage reward: 19.40, std dev: 10.95, max: 59.00\n",
      "Episode 4000\taverage reward: 21.70, std dev: 9.90, max: 54.00\n",
      "Episode 4250\taverage reward: 22.33, std dev: 10.91, max: 64.00\n",
      "Episode 4500\taverage reward: 22.86, std dev: 12.91, max: 83.00\n",
      "Episode 4750\taverage reward: 19.56, std dev: 9.78, max: 56.00\n",
      "Episode 5000\taverage reward: 21.59, std dev: 11.49, max: 62.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "q_ac_agent = QActorCritic(env)\n",
    "episode_rewards = q_ac_agent.train(num_episodes=int(5e3), max_steps=200, batch_size=12, log_interval=250, solve_score=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done status:  True\n",
      "step taken (maximum 500):  11\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent in 1 episode\n",
    "trajectory, done = q_ac_agent.get_trajectory(max_steps=501)\n",
    "print(\"done status: \", done)\n",
    "print(\"step taken (maximum 500): \", len(trajectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 0.00% from 100 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "test_episodes = 100\n",
    "win_rate = q_ac_agent.evaluate_policy(num_episodes=test_episodes)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the trained agent on a separate window\n",
    "q_ac_agent.visualize_policy(num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
