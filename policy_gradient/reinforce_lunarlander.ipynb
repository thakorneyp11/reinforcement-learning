{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Environment Information\n",
    "ref: https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py#L75\n",
    "\n",
    "Action Space\n",
    "    There are four discrete actions available: do nothing, fire left\n",
    "    orientation engine, fire main engine, fire right orientation engine.\n",
    "Observation Space\n",
    "    The state is an 8-dimensional vector: the coordinates of the lander in `x` & `y`, its linear\n",
    "    velocities in `x` & `y`, its angle, its angular velocity, and two booleans\n",
    "    that represent whether each leg is in contact with the ground or not.\n",
    "Rewards\n",
    "    For each step, the reward:\n",
    "    - is increased/decreased the closer/further the lander is to the landing pad.\n",
    "    - is increased/decreased the slower/faster the lander is moving.\n",
    "    - is decreased the more the lander is tilted (angle not horizontal).\n",
    "    - is increased by 10 points for each leg that is in contact with the ground.\n",
    "    - is decreased by 0.03 points each frame a side engine is firing.\n",
    "    - is decreased by 0.3 points each frame the main engine is firing.\n",
    "    The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "    An episode is considered a solution if it scores at least 200 points.\n",
    "\"\"\"\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample environment image\n",
    "\n",
    "<img width=300 src=\"lunar_lander.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space:  Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)\n",
      "action_space:  Discrete(4)\n",
      "sample obs:  [-0.00464611  1.4173794  -0.47061905  0.2870732   0.00539051  0.10660225\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_space: \", env.observation_space)\n",
    "print(\"action_space: \", env.action_space)\n",
    "state = env.reset()\n",
    "print(\"sample obs: \", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample step:  (array([ 2.9697418e-04,  1.4097712e+00,  9.2715714e-03, -3.8273171e-02,\n",
      "        1.5079111e-03,  3.4357999e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "      dtype=float32), -1.728385070643468, False, {})\n"
     ]
    }
   ],
   "source": [
    "# sample step\n",
    "new_state, reward, done, info = env.step(1)\n",
    "print(\"sample step: \", (new_state, reward, done, info))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REINFORCE: Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(VanillaNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=-1)  # dim need to be -1 to prevent NaN results\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2580, 0.2447, 0.2471, 0.2503], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample_nn = VanillaNN(n_observations=8, n_actions=4)\n",
    "pred = sample_nn(torch.tensor(state).float())\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce:\n",
    "    def __init__(self, env, gamma=0.99, learning_rate=3e-3):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # get number of actions and observations\n",
    "        self._n_observations = self.env.observation_space.shape[0]\n",
    "        self._n_actions = self.env.action_space.n\n",
    "\n",
    "        # setup NN model\n",
    "        self.policy_net = VanillaNN(self._n_observations, self._n_actions).to(device)\n",
    "\n",
    "        # setup optimizer and loss function\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = self.loss_function\n",
    "        \n",
    "    def loss_function(self, prob_batch, expected_returns_batch):\n",
    "        return - torch.sum(torch.log(prob_batch) * expected_returns_batch)\n",
    "\n",
    "    def get_trajectory(self, max_steps=500, render=False):\n",
    "        trajectory = list()\n",
    "        done = False  # incase of early loop-termination (max_steps) before the environment terminated\n",
    "        state = self.env.reset()\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action_probs = self.policy_net(torch.from_numpy(state).float())\n",
    "            selected_action = np.random.choice(np.array([0, 1, 2, 3]), p=action_probs.data.numpy())\n",
    "            next_state, reward, done, _ = self.env.step(selected_action)\n",
    "            trajectory.append((state, selected_action, reward))\n",
    "            prev_state = state\n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return trajectory, done\n",
    "\n",
    "    def train(self, num_episodes=1000, max_steps=500):\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            # get sample trajectory from the policy network\n",
    "            trajectory, _ = self.get_trajectory(max_steps=max_steps)\n",
    "            \n",
    "            # prepare data for training\n",
    "            states, actions, rewards = zip(*trajectory)\n",
    "            states = torch.Tensor(states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            \n",
    "            # calculate expected discounted returns\n",
    "            expected_returns_batch = list()\n",
    "            for idx in range(len(rewards)):\n",
    "                discounted_rewards = [self.gamma**i * reward for i, reward in enumerate(rewards[idx:])]\n",
    "                expected_returns_batch.append(sum(discounted_rewards))\n",
    "            \n",
    "            # normalize and reformat expected returns\n",
    "            expected_returns_batch = torch.FloatTensor(expected_returns_batch)\n",
    "            expected_returns_batch /= expected_returns_batch.max()\n",
    "            \n",
    "            # calculate loss\n",
    "            action_probs = self.policy_net(states)\n",
    "            prob_batch = action_probs.gather(dim=1,index=actions.long().view(-1,1)).squeeze() \n",
    "            loss = self.criterion(prob_batch, expected_returns_batch)\n",
    "            \n",
    "            # optimize the model with backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "    def visualize_policy(self, num_episodes=1, max_steps=500):\n",
    "        for _ in range(num_episodes):\n",
    "            trajectory, done = self.get_trajectory(max_steps=max_steps, render=True)\n",
    "            \n",
    "    def evaluate_policy(self, num_episodes=100, max_steps=500):\n",
    "        win = 0\n",
    "        for _ in range(num_episodes):\n",
    "            trajectory, done = self.get_trajectory(max_steps=max_steps)\n",
    "            if done and trajectory[-1][2] == 100:\n",
    "                win += 1\n",
    "        return win / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [58:03<00:00, 28.71it/s] \n"
     ]
    }
   ],
   "source": [
    "# TODO: need to train more\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "reinforce_agent = Reinforce(env)\n",
    "reinforce_agent.train(num_episodes=int(5e6), max_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 10.00% from 100 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "test_episodes = 100\n",
    "win_rate = reinforce_agent.evaluate_policy(num_episodes=test_episodes)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the trained agent on a separate window\n",
    "reinforce_agent.visualize_policy(num_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
