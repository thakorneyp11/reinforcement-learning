{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "\n",
    "from algorithms import Reinforce\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Environment Information\n",
    "ref: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L17\n",
    "\n",
    "Action Space\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "Observation Space\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "Rewards\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
    "    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "Episode End\n",
    "    The episode ends if any one of the following occurs:\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\"\"\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample environment image\n",
    "\n",
    "<img width=300 src=\"cartpole.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action_space:  Discrete(2)\n",
      "sample obs:  [-0.04331493  0.01159795 -0.01566737  0.01882051]\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_space: \", env.observation_space)\n",
    "print(\"action_space: \", env.action_space)\n",
    "state = env.reset()\n",
    "print(\"sample obs: \", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample step:  (array([-0.04308297, -0.18329585, -0.01529096,  0.3065193 ], dtype=float32), 1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# sample step\n",
    "new_state, reward, done, info = env.step(0)\n",
    "print(\"sample step: \", (new_state, reward, done, info))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REINFORCE: Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 134.83it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "reinforce_agent = Reinforce(env)\n",
    "reinforce_agent.train(num_episodes=int(500), max_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done status:  True\n",
      "step taken (maximum 500):  500\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent in 1 episode\n",
    "trajectory, done = reinforce_agent.get_trajectory(max_steps=501)\n",
    "print(\"done status: \", done)\n",
    "print(\"step taken (maximum 500): \", len(trajectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 98.00% from 100 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "def evaluate_policy(agent, num_episodes=100, max_steps=500):\n",
    "    win = 0\n",
    "    for _ in range(num_episodes):\n",
    "        trajectory, _ = agent.get_trajectory(max_steps=max_steps)\n",
    "        # CUSTOMIZE: for cartpole environment\n",
    "        if len(trajectory) >= 490:\n",
    "            win += 1\n",
    "    return win / num_episodes\n",
    "\n",
    "test_episodes = 100\n",
    "win_rate = evaluate_policy(reinforce_agent, num_episodes=test_episodes)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done status:  True\n",
      "step taken (maximum 500):  500\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent in 1 episode\n",
    "trajectory, done = reinforce_agent.get_trajectory(max_steps=501)\n",
    "print(\"done status: \", done)\n",
    "print(\"step taken (maximum 500): \", len(trajectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the trained agent on a separate window\n",
    "reinforce_agent.visualize_policy(num_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
