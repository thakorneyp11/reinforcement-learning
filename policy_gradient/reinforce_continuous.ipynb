{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "# allow PyTorch throw errors as soon as a NaN gradient is detected\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# if GPU is to be used\n",
    "\"\"\" ref: M1 GPU support\n",
    "https://developer.apple.com/metal/pytorch/\n",
    "https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\n",
    "\"\"\"\n",
    "_device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_device_name = \"mps\" if torch.backends.mps.is_available() else _device_name\n",
    "device = torch.device(_device_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MountainCar-Continuous Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Environment information\n",
    "ref: https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py#L27\n",
    "\n",
    "Observation Space\n",
    "    The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
    "    | Num | Observation                          | Min  | Max | Unit         |\n",
    "    |-----|--------------------------------------|------|-----|--------------|\n",
    "    | 0   | position of the car along the x-axis | -Inf | Inf | position (m) |\n",
    "    | 1   | velocity of the car                  | -Inf | Inf | position (m) |\n",
    "Action Space\n",
    "    The action is a `ndarray` with shape `(1,)`, representing the directional force applied on the car.\n",
    "    The action is clipped in the range `[-1,1]` and multiplied by a power of 0.0015.\n",
    "Reward\n",
    "    A negative reward of *-0.1 * action<sup>2</sup>* is received at each timestep to penalise for\n",
    "    taking actions of large magnitude. If the mountain car reaches the goal then a positive reward of +100\n",
    "    is added to the negative reward for that timestep.\n",
    "Starting State\n",
    "    The position of the car is assigned a uniform random value in `[-0.6 , -0.4]`.\n",
    "    The starting velocity of the car is always assigned to 0.\n",
    "Episode End\n",
    "    The episode ends if either of the following happens:\n",
    "    1. Termination: The position of the car is greater than or equal to 0.45 (the goal position on top of the right hill)\n",
    "    2. Truncation: The length of the episode is 999.\n",
    "\"\"\"\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode=\"human\")\n",
    "# env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 23:04:35.393 Python[97060:4524954] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/rd/xnkdmhqx0c19zv0s0z9zcc9c0000gn/T/org.python.python.savedState\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.5914051,  0.       ],\n",
       "       [-0.5914051,  0.       ],\n",
       "       [-0.5914051,  0.       ],\n",
       "       [-0.5914051,  0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class ConcatObs(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=((k,) + shp), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self, options=None):\n",
    "        ob, _ = self.env.reset(options=options)\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, _, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.array(self.frames)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode=\"human\")\n",
    "concat_env = ConcatObs(env, 4)\n",
    "concat_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample environment image\n",
    "\n",
    "<img width=300 src=\"mountain_car.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space:  Box([[-inf -inf]\n",
      " [-inf -inf]\n",
      " [-inf -inf]\n",
      " [-inf -inf]], [[inf inf]\n",
      " [inf inf]\n",
      " [inf inf]\n",
      " [inf inf]], (4, 2), float32)\n",
      "sample obs:  [[0.08869325 0.        ]\n",
      " [0.08869325 0.        ]\n",
      " [0.08869325 0.        ]\n",
      " [0.08869325 0.        ]]\n",
      "action_space:  Box(-1.0, 1.0, (1,), float32)\n",
      "sample action:  [-0.21842176]\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_space: \", concat_env.observation_space)\n",
    "\n",
    "obs = concat_env.reset(options={\"low\": -1.0, \"high\": 0.3})  # set random initial position [-1, 0.3]\n",
    "print(\"sample obs: \", obs)\n",
    "\n",
    "print(\"action_space: \", concat_env.action_space)\n",
    "print(\"sample action: \", concat_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample step:  (array([ 0.08576267, -0.00293058], dtype=float32), -0.011951178394061035, False, {})\n"
     ]
    }
   ],
   "source": [
    "# sample step\n",
    "new_state, reward, done, _, info = env.step(env.action_space.sample())\n",
    "print(\"sample step: \", (new_state, reward, done, info))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REINFORCE with continuous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(VanillaNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.mu_layer = nn.Linear(128, n_actions)\n",
    "        self.sigma_layer = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten 2x2D array to 1D array first\n",
    "        x = x.flatten()\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        mu = self.mu_layer(x)\n",
    "        sigma = torch.exp(self.sigma_layer(x))  # Ensure positive standard deviation\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  [[-0.5267669  0.       ]\n",
      " [-0.5267669  0.       ]\n",
      " [-0.5267669  0.       ]\n",
      " [-0.5267669  0.       ]]\n",
      "mu pred: tensor([-0.1702], grad_fn=<AddBackward0>), sigma pred: tensor([0.9718], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample_nn = VanillaNN(concat_env.observation_space.shape[0]*concat_env.observation_space.shape[1], concat_env.action_space.shape[0])\n",
    "\n",
    "state = concat_env.reset()\n",
    "_state = torch.tensor(state, dtype=float).float()\n",
    "\n",
    "mu, sigma = sample_nn(_state)\n",
    "\n",
    "print(\"state: \", state)\n",
    "print(\"mu pred: {}, sigma pred: {}\".format(mu, sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceContinuous:\n",
    "    def __init__(self, env, gamma=0.99, learning_rate=1e-3):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # get number of actions and observations\n",
    "        self._n_observations = self.env.observation_space.shape[0] * self.env.observation_space.shape[1]\n",
    "        self._n_actions = self.env.action_space.shape[0]\n",
    "\n",
    "        # setup NN model\n",
    "        self.policy_net = VanillaNN(self._n_observations, self._n_actions).to(device)\n",
    "\n",
    "        # setup optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.esp = np.finfo(np.float32).eps.item() # prevent division by zero\n",
    "        \n",
    "        # set random initial position to improve exploration\n",
    "        self.initial_options = {\"low\": -1.0, \"high\": 0.3}  # default: {\"low\": -0.6, \"high\": -0.4}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # forward pass to get estimate parameters (mu and sigma) of 1 action\n",
    "        _state = torch.from_numpy(state).float().to(device)\n",
    "        mu, sigma = self.policy_net(_state)\n",
    "        \n",
    "        if torch.isnan(mu).any() or torch.isnan(sigma).any():\n",
    "            print(\"found erorr at state =\", _state)\n",
    "            print(\"NaN detected in mu or sigma\")\n",
    "        \n",
    "        if mu is None or sigma is None:\n",
    "            print(\"found erorr at state =\", _state)\n",
    "            print(\"mu: {}, sigma: {}\".format(mu, sigma))\n",
    "        \n",
    "        # calculate normal distribution from mean and std\n",
    "        m = Normal(mu, torch.exp(sigma) + self.esp)\n",
    "        \n",
    "        # sample action and calculate log probability\n",
    "        selected_action = m.sample()\n",
    "        log_prop = m.log_prob(selected_action)\n",
    "        \n",
    "        return selected_action, log_prop\n",
    "\n",
    "    def get_trajectory(self, max_steps=500, render=False):\n",
    "        trajectory = list()\n",
    "        done = False  # incase of early loop-termination (max_steps) before the environment terminated\n",
    "        state = self.env.reset(options=self.initial_options)\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            # choose and take action based on normal distribution\n",
    "            _selected_action, log_prop = self.choose_action(state)\n",
    "            \n",
    "            # apply clip to action to ensure it is within the action space\n",
    "            selected_action = np.clip(_selected_action.item(), -1, 1)\n",
    "            \n",
    "            next_state, reward, done, _ = self.env.step([selected_action])\n",
    "            trajectory.append((state, selected_action, reward, log_prop))\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return trajectory, done\n",
    "\n",
    "    def train(self, num_episodes=1000, max_steps=500, SEED=123):\n",
    "        self.env.action_space.seed(SEED)\n",
    "        _ = torch.manual_seed(SEED)\n",
    "        self.num_returns = 0\n",
    "        self.sum_returns = 0\n",
    "        self.sum_returns_squared = 0\n",
    "        \n",
    "        # track recent last return to identify if the environment is solved\n",
    "        running_rewards = deque(maxlen=100)\n",
    "        \n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            # get sample trajectory from the policy network\n",
    "            trajectory, _ = self.get_trajectory(max_steps=max_steps)\n",
    "            \n",
    "            # prepare data for training\n",
    "            states, actions, rewards, log_props = zip(*trajectory)\n",
    "            states = torch.Tensor(np.array(states))\n",
    "            actions = torch.Tensor(np.array(actions))\n",
    "            log_props = torch.stack(log_props)\n",
    "            \n",
    "            # calculate expected discounted returns\n",
    "            expected_returns_batch = list()\n",
    "            for idx in range(len(rewards)):\n",
    "                discounted_rewards = [self.gamma**i * reward for i, reward in enumerate(rewards[idx:])]\n",
    "                expected_returns_batch.append(sum(discounted_rewards))\n",
    "            \n",
    "            # normalize and reformat expected returns\n",
    "            expected_returns = torch.FloatTensor(expected_returns_batch)\n",
    "\n",
    "            self.num_returns += len(expected_returns)\n",
    "            self.sum_returns += sum(expected_returns)\n",
    "            self.sum_returns_squared += sum(expected_returns**2)\n",
    "            \n",
    "            _return_mean = self.sum_returns / self.num_returns\n",
    "            _return_std_dev = np.sqrt(self.sum_returns_squared / self.num_returns - _return_mean**2)\n",
    "            normalized_expected_returns = (expected_returns - _return_mean) / (_return_std_dev + self.esp)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = - torch.sum(log_props * normalized_expected_returns.to(device))\n",
    "            \n",
    "            # log results\n",
    "            _episode_reward = sum(rewards)\n",
    "            running_rewards.append(_episode_reward)\n",
    "            running_mean = np.array(running_rewards).mean()\n",
    "            running_std_dev = np.array(running_rewards).std()\n",
    "            running_max = np.array(running_rewards).max()\n",
    "            if (episode+1) % 25 == 0:\n",
    "                print(f\"Episode {episode+1}\\taverage reward: {running_mean:.2f}, std dev: {running_std_dev:.2f}, max: {running_max:.2f}\")\n",
    "                \n",
    "            if running_mean > 75 and len(running_rewards) >= 100:\n",
    "                print(f\"Solved! Running reward is now {running_mean:.2f}\")\n",
    "                print(f\"Episode {episode+1}\\taverage reward: {running_mean:.2f}, std dev: {running_std_dev:.2f}, max: {running_max:.2f}\")\n",
    "                break\n",
    "            \n",
    "            # optimize the model with backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "    def visualize_policy(self, num_episodes=1, max_steps=500):\n",
    "        for _ in range(num_episodes):\n",
    "            trajectory, done = self.get_trajectory(max_steps=max_steps, render=True)\n",
    "            \n",
    "    def evaluate_policy(self, num_episodes=100, max_steps=500):\n",
    "        win = 0\n",
    "        for _ in range(num_episodes):\n",
    "            trajectory, done = self.get_trajectory(max_steps=max_steps)\n",
    "            if done and trajectory[-1][2] >= 80:\n",
    "                win += 1\n",
    "        return win / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1000 [08:47<5:54:57, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25\taverage reward: -36.26, std dev: 26.06, max: 91.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 49/1000 [18:06<6:06:14, 23.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\taverage reward: -32.95, std dev: 31.97, max: 91.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 74/1000 [27:04<5:57:49, 23.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 75\taverage reward: -28.18, std dev: 38.48, max: 92.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 99/1000 [36:13<5:47:36, 23.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\taverage reward: -28.11, std dev: 38.56, max: 92.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 124/1000 [44:29<4:44:18, 19.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 125\taverage reward: -19.89, std dev: 46.74, max: 92.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 149/1000 [53:20<5:13:01, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 150\taverage reward: -18.15, std dev: 47.64, max: 92.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 174/1000 [1:01:32<3:41:42, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 175\taverage reward: -14.11, std dev: 50.36, max: 92.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 199/1000 [1:10:18<5:05:16, 22.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200\taverage reward: -9.37, std dev: 52.72, max: 92.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 224/1000 [1:18:56<3:59:00, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 225\taverage reward: -13.16, std dev: 50.58, max: 92.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 249/1000 [1:27:29<3:44:04, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250\taverage reward: -10.37, std dev: 52.79, max: 92.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 274/1000 [1:35:38<4:08:37, 20.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 275\taverage reward: -10.57, std dev: 52.44, max: 91.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 299/1000 [1:44:23<4:23:27, 22.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300\taverage reward: -12.42, std dev: 51.31, max: 92.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 324/1000 [1:53:32<4:13:02, 22.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 325\taverage reward: -13.23, std dev: 49.95, max: 92.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 349/1000 [2:03:05<4:11:57, 23.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 350\taverage reward: -19.14, std dev: 44.69, max: 92.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 374/1000 [2:11:04<2:48:31, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 375\taverage reward: -17.44, std dev: 46.08, max: 93.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 399/1000 [2:17:29<2:39:23, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400\taverage reward: -2.53, std dev: 55.42, max: 94.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 424/1000 [2:24:14<2:25:02, 15.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 425\taverage reward: 14.23, std dev: 59.49, max: 94.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 449/1000 [2:30:22<2:16:22, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 450\taverage reward: 36.79, std dev: 57.28, max: 94.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 474/1000 [2:36:26<2:24:36, 16.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 475\taverage reward: 50.04, std dev: 51.70, max: 94.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 499/1000 [2:42:58<2:02:47, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500\taverage reward: 52.82, std dev: 48.98, max: 93.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 524/1000 [2:49:39<2:36:28, 19.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 525\taverage reward: 52.00, std dev: 49.90, max: 93.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 549/1000 [2:56:54<2:21:56, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 550\taverage reward: 44.28, std dev: 54.23, max: 93.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 574/1000 [3:02:30<2:04:05, 17.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 575\taverage reward: 47.02, std dev: 53.41, max: 96.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 599/1000 [3:09:06<1:40:48, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600\taverage reward: 45.15, std dev: 54.38, max: 96.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 624/1000 [3:14:24<1:39:42, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 625\taverage reward: 49.99, std dev: 53.10, max: 96.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 649/1000 [3:20:03<1:09:01, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 650\taverage reward: 60.75, std dev: 45.85, max: 96.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 674/1000 [3:24:25<1:06:56, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 675\taverage reward: 64.52, std dev: 42.79, max: 93.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 699/1000 [3:28:36<47:57,  9.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700\taverage reward: 72.75, std dev: 35.11, max: 96.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 713/1000 [3:30:22<1:24:40, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved! Running reward is now 75.33\n",
      "Episode 714\taverage reward: 75.33, std dev: 31.65, max: 96.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\")\n",
    "concat_env = ConcatObs(env, 8)\n",
    "reinforce_agent = ReinforceContinuous(concat_env)\n",
    "reinforce_agent.train(num_episodes=int(1e3), max_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 100.00% from 10 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "test_episodes = 10\n",
    "win_rate = reinforce_agent.evaluate_policy(num_episodes=test_episodes, max_steps=999)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the trained agent on a separate window\n",
    "reinforce_agent.visualize_policy(num_episodes=10, max_steps=999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
