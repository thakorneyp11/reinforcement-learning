{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Environment Information\n",
    "ref: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L17\n",
    "\n",
    "Action Space\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "Observation Space\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "Rewards\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
    "    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "Episode End\n",
    "    The episode ends if any one of the following occurs:\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\"\"\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample environment image\n",
    "\n",
    "<img width=300 src=\"cartpole.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action_space:  Discrete(2)\n",
      "sample obs:  [-0.04331493  0.01159795 -0.01566737  0.01882051]\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_space: \", env.observation_space)\n",
    "print(\"action_space: \", env.action_space)\n",
    "state = env.reset()\n",
    "print(\"sample obs: \", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample step:  (array([-0.04308297, -0.18329585, -0.01529096,  0.3065193 ], dtype=float32), 1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# sample step\n",
    "new_state, reward, done, info = env.step(0)\n",
    "print(\"sample step: \", (new_state, reward, done, info))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REINFORCE: Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(VanillaNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=-1)  # dim need to be -1 to prevent NaN results\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce:\n",
    "    def __init__(self, env, gamma=0.99, learning_rate=3e-3, device=\"cpu\"):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "\n",
    "        # get number of actions and observations\n",
    "        self._n_observations = self.env.observation_space.shape[0]\n",
    "        self._n_actions = self.env.action_space.n\n",
    "\n",
    "        # setup NN model\n",
    "        self.policy_net = VanillaNN(self._n_observations, self._n_actions).to(self.device)\n",
    "\n",
    "        # setup optimizer and loss function\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = self.loss_function\n",
    "        \n",
    "    def loss_function(self, prob_batch, expected_returns_batch):\n",
    "        return - torch.sum(torch.log(prob_batch) * expected_returns_batch)\n",
    "\n",
    "    def get_trajectory(self, max_steps=500, render=False):\n",
    "        trajectory = list()\n",
    "        done = False  # incase of early loop-termination (max_steps) before the environment terminated\n",
    "        state = self.env.reset()\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action_probs = self.policy_net(torch.from_numpy(state).float())\n",
    "            # TODO: support continuous action space\n",
    "            selected_action = np.random.choice(np.array(range(self._n_actions)), p=action_probs.data.numpy())\n",
    "            next_state, reward, done, _ = self.env.step(selected_action)\n",
    "            trajectory.append((state, selected_action, reward))\n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return trajectory, done\n",
    "\n",
    "    def train(self, num_episodes=1000, max_steps=500):\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            # get sample trajectory from the policy network\n",
    "            trajectory, _ = self.get_trajectory(max_steps=max_steps)\n",
    "            \n",
    "            # prepare data for training\n",
    "            states, actions, rewards = zip(*trajectory)\n",
    "            states = torch.Tensor(states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            \n",
    "            # calculate expected discounted returns\n",
    "            expected_returns_batch = list()\n",
    "            for idx in range(len(rewards)):\n",
    "                discounted_rewards = [self.gamma**i * reward for i, reward in enumerate(rewards[idx:])]\n",
    "                expected_returns_batch.append(sum(discounted_rewards))\n",
    "            \n",
    "            # normalize and reformat expected returns\n",
    "            expected_returns_batch = torch.FloatTensor(expected_returns_batch)\n",
    "            expected_returns_batch /= expected_returns_batch.max()\n",
    "            \n",
    "            # calculate loss\n",
    "            action_probs = self.policy_net(states)\n",
    "            prob_batch = action_probs.gather(dim=1,index=actions.long().view(-1,1)).squeeze() \n",
    "            loss = self.criterion(prob_batch, expected_returns_batch)\n",
    "            \n",
    "            # optimize the model with backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "    def visualize_policy(self, num_episodes=1, max_steps=500):\n",
    "        for _ in range(num_episodes):\n",
    "            trajectory, done = self.get_trajectory(max_steps=max_steps, render=True)\n",
    "            \n",
    "    def evaluate_policy(self, num_episodes=100, max_steps=500):\n",
    "        win = 0\n",
    "        for _ in range(num_episodes):\n",
    "            trajectory, done = self.get_trajectory(max_steps=max_steps)\n",
    "            # CUSTOMIZE: for lunarlander environment\n",
    "            if done and trajectory[-1][2] == 100:\n",
    "                win += 1\n",
    "        return win / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 134.83it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "reinforce_agent = Reinforce(env)\n",
    "reinforce_agent.train(num_episodes=int(500), max_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done status:  True\n",
      "step taken (maximum 500):  500\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent in 1 episode\n",
    "trajectory, done = reinforce_agent.get_trajectory(max_steps=501)\n",
    "print(\"done status: \", done)\n",
    "print(\"step taken (maximum 500): \", len(trajectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 98.00% from 100 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "def evaluate_policy(agent, num_episodes=100, max_steps=500):\n",
    "    win = 0\n",
    "    for _ in range(num_episodes):\n",
    "        trajectory, _ = agent.get_trajectory(max_steps=max_steps)\n",
    "        # CUSTOMIZE: for cartpole environment\n",
    "        if len(trajectory) >= 490:\n",
    "            win += 1\n",
    "    return win / num_episodes\n",
    "\n",
    "test_episodes = 100\n",
    "win_rate = evaluate_policy(reinforce_agent, num_episodes=test_episodes)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the trained agent on a separate window\n",
    "reinforce_agent.visualize_policy(num_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
