{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Environment Information\n",
    "ref: https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py#L75\n",
    "\n",
    "Action Space\n",
    "    There are four discrete actions available: do nothing, fire left\n",
    "    orientation engine, fire main engine, fire right orientation engine.\n",
    "Observation Space\n",
    "    The state is an 8-dimensional vector: the coordinates of the lander in `x` & `y`, its linear\n",
    "    velocities in `x` & `y`, its angle, its angular velocity, and two booleans\n",
    "    that represent whether each leg is in contact with the ground or not.\n",
    "Rewards\n",
    "    For each step, the reward:\n",
    "    - is increased/decreased the closer/further the lander is to the landing pad.\n",
    "    - is increased/decreased the slower/faster the lander is moving.\n",
    "    - is decreased the more the lander is tilted (angle not horizontal).\n",
    "    - is increased by 10 points for each leg that is in contact with the ground.\n",
    "    - is decreased by 0.03 points each frame a side engine is firing.\n",
    "    - is decreased by 0.3 points each frame the main engine is firing.\n",
    "    The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "    An episode is considered a solution if it scores at least 200 points.\n",
    "\"\"\"\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample environment image\n",
    "\n",
    "<img width=300 src=\"lunar_lander.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space:  Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)\n",
      "action_space:  Discrete(4)\n",
      "sample obs:  [ 0.00572481  1.402558    0.57985747 -0.37165737 -0.00662694 -0.13134624\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_space: \", env.observation_space)\n",
    "print(\"action_space: \", env.action_space)\n",
    "state = env.reset()\n",
    "print(\"sample obs: \", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample step:  (array([ 0.01137152,  1.3936093 ,  0.5692361 , -0.39774814, -0.01114556,\n",
      "       -0.09038049,  0.        ,  0.        ], dtype=float32), -0.1594018753271473, False, {})\n"
     ]
    }
   ],
   "source": [
    "# sample step\n",
    "new_state, reward, done, info = env.step(1)\n",
    "print(\"sample step: \", (new_state, reward, done, info))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REINFORCE: Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(VanillaNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 64)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.layer2 = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(self.dropout(x))\n",
    "        x = F.softmax(self.layer2(x), dim=-1)  # dim need to be -1 to prevent NaN results\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2157, 0.3474, 0.2500, 0.1869], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample_nn = VanillaNN(n_observations=8, n_actions=4)\n",
    "pred = sample_nn(torch.tensor(state).float())\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce:\n",
    "    def __init__(self, env, gamma=0.99, learning_rate=1e-2):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # get number of actions and observations\n",
    "        self._n_observations = self.env.observation_space.shape[0]\n",
    "        self._n_actions = self.env.action_space.n\n",
    "\n",
    "        # setup NN model\n",
    "        self.policy_net = VanillaNN(self._n_observations, self._n_actions).to(device)\n",
    "\n",
    "        # setup optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.esp = np.finfo(np.float32).eps.item() # prevent division by zero\n",
    "\n",
    "    def get_trajectory(self, max_steps=500, render=False):\n",
    "        trajectory = list()\n",
    "        done = False  # incase of early loop-termination (max_steps) before the environment terminated\n",
    "        state = self.env.reset()\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            _state = torch.from_numpy(state).float()\n",
    "            action_probs = self.policy_net(_state)\n",
    "            m = Categorical(action_probs)\n",
    "            selected_action = m.sample()\n",
    "            log_prop = m.log_prob(selected_action)\n",
    "            next_state, reward, done, _ = self.env.step(selected_action.item())\n",
    "            trajectory.append((state, selected_action, reward, log_prop))\n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return trajectory, done\n",
    "\n",
    "    def train(self, num_episodes=1000, max_steps=500, SEED=123):\n",
    "        self.env.seed(SEED)\n",
    "        _ = torch.manual_seed(SEED)\n",
    "        self.num_returns = 0\n",
    "        self.sum_returns = 0\n",
    "        self.sum_returns_squared = 0\n",
    "\n",
    "        running_rewards = deque(maxlen=100)\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # get sample trajectory from the policy network\n",
    "            trajectory, _ = self.get_trajectory(max_steps=max_steps)\n",
    "            \n",
    "            # prepare data for training\n",
    "            states, actions, rewards, log_props = zip(*trajectory)\n",
    "            states = torch.Tensor(states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            log_props = torch.stack(log_props)\n",
    "            \n",
    "            # calculate expected discounted returns\n",
    "            expected_returns_batch = list()\n",
    "            for idx in range(len(rewards)):\n",
    "                discounted_rewards = [self.gamma**i * reward for i, reward in enumerate(rewards[idx:])]\n",
    "                expected_returns_batch.append(sum(discounted_rewards))\n",
    "            \n",
    "            # normalize and reformat expected returns\n",
    "            expected_returns = torch.tensor(expected_returns_batch)\n",
    "            \n",
    "            self.num_returns += len(expected_returns)\n",
    "            self.sum_returns += sum(expected_returns)\n",
    "            self.sum_returns_squared += sum(expected_returns**2)\n",
    "            \n",
    "            _return_mean = self.sum_returns / self.num_returns\n",
    "            _return_std_dev = np.sqrt(self.sum_returns_squared / self.num_returns - _return_mean**2)\n",
    "            normalized_expected_returns = (expected_returns - _return_mean) / (_return_std_dev + self.esp)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = - torch.sum(log_props * normalized_expected_returns)\n",
    "            \n",
    "            # log results\n",
    "            _episode_reward = sum(rewards)\n",
    "            running_rewards.append(_episode_reward)\n",
    "            running_mean = np.array(running_rewards).mean()\n",
    "            running_std_dev = np.array(running_rewards).std()\n",
    "            if (episode+1) % 100 == 0:\n",
    "                print(f\"Episode {episode+1}\\taverage reward: {running_mean:.2f}, std dev: {running_std_dev:.2f}\")\n",
    "                \n",
    "            if running_mean > 150:\n",
    "                print(f\"Solved! Running reward is now {running_mean:.2f}\")\n",
    "                print(f\"Episode {episode+1}\\taverage reward: {running_mean:.2f}, std dev: {running_std_dev:.2f}\")\n",
    "                break\n",
    "            \n",
    "            # optimize the model with backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "    def visualize_policy(self, num_episodes=1, max_steps=500):\n",
    "        for _ in range(num_episodes):\n",
    "            trajectory, done = self.get_trajectory(max_steps=max_steps, render=True)\n",
    "            \n",
    "    def evaluate_policy(self, num_episodes=100, max_steps=500):\n",
    "        win = 0\n",
    "        for _ in range(num_episodes):\n",
    "            trajectory, done = self.get_trajectory(max_steps=max_steps)\n",
    "            if done and trajectory[-1][2] == 100:\n",
    "                win += 1\n",
    "        return win / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\taverage reward: -170.52, std dev: 93.68\n",
      "Episode 200\taverage reward: -127.20, std dev: 118.32\n",
      "Episode 300\taverage reward: -75.02, std dev: 68.39\n",
      "Episode 400\taverage reward: -51.39, std dev: 50.13\n",
      "Episode 500\taverage reward: -49.71, std dev: 71.61\n",
      "Episode 600\taverage reward: -32.07, std dev: 46.09\n",
      "Episode 700\taverage reward: -4.18, std dev: 44.44\n",
      "Episode 800\taverage reward: 36.79, std dev: 100.43\n",
      "Episode 900\taverage reward: 3.80, std dev: 88.15\n",
      "Episode 1000\taverage reward: 33.73, std dev: 111.47\n",
      "Episode 1100\taverage reward: 4.97, std dev: 121.44\n",
      "Episode 1200\taverage reward: -6.69, std dev: 117.71\n",
      "Episode 1300\taverage reward: 45.07, std dev: 84.62\n",
      "Episode 1400\taverage reward: 72.18, std dev: 68.44\n",
      "Episode 1500\taverage reward: 69.10, std dev: 78.80\n",
      "Episode 1600\taverage reward: 84.25, std dev: 67.69\n",
      "Episode 1700\taverage reward: 102.80, std dev: 67.47\n",
      "Solved! Running reward is now 150.96\n",
      "Episode 1765\taverage reward: 150.96, std dev: 99.37\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "reinforce_agent = Reinforce(env)\n",
    "reinforce_agent.train(num_episodes=int(3e3), max_steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 94.00% from 100 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "test_episodes = 100\n",
    "win_rate = reinforce_agent.evaluate_policy(num_episodes=test_episodes, max_steps=5000)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the trained agent on a separate window\n",
    "reinforce_agent.visualize_policy(num_episodes=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. REINFORCE with adaptive Baseline (state-value function)\n",
    "ref: https://medium.com/nerd-for-tech/policy-gradients-reinforce-with-baseline-6c871a3a068\n",
    "\n",
    "ref: https://github.com/riccardocadei/LunarLander-v2-REINFORCE/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunctionNN(nn.Module):\n",
    "    def __init__(self, n_observations):\n",
    "        super(ValueFunctionNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1136], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample_nn = ValueFunctionNN(n_observations=8)\n",
    "pred = sample_nn(torch.tensor(state).float())\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceWithBaseline(Reinforce):\n",
    "    def __init__(self, env, gamma=0.99, learning_rate=1e-2):\n",
    "        super(ReinforceWithBaseline, self).__init__(env, gamma, learning_rate)\n",
    "        \n",
    "        # setup state-value function network and optimizer\n",
    "        self.value_net = ValueFunctionNN(self._n_observations).to(device)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def train(self, num_episodes=1000, max_steps=500, SEED=123):\n",
    "        self.env.seed(SEED)\n",
    "        _ = torch.manual_seed(SEED)\n",
    "        self.num_returns = 0\n",
    "        self.sum_returns = 0\n",
    "        self.sum_returns_squared = 0\n",
    "\n",
    "        running_rewards = deque(maxlen=100)\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # get sample trajectory from the policy network\n",
    "            trajectory, _ = self.get_trajectory(max_steps=max_steps)\n",
    "            \n",
    "            # prepare data for training\n",
    "            states, actions, rewards, log_props = zip(*trajectory)\n",
    "            states = torch.Tensor(states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            log_props = torch.stack(log_props)\n",
    "            \n",
    "            # calculate expected discounted returns\n",
    "            expected_returns_batch = list()\n",
    "            for idx in range(len(rewards)):\n",
    "                discounted_rewards = [self.gamma**i * reward for i, reward in enumerate(rewards[idx:])]\n",
    "                expected_returns_batch.append(sum(discounted_rewards))\n",
    "            \n",
    "            # normalize and reformat expected returns\n",
    "            expected_returns = torch.tensor(expected_returns_batch)\n",
    "            \n",
    "            # calculate advantage function from value function\n",
    "            values = self.value_net(states)\n",
    "            advantages = expected_returns - values.squeeze().detach()\n",
    "\n",
    "            # calculate loss\n",
    "            loss = - torch.sum(log_props * advantages)\n",
    "            \n",
    "            # update value function with MSE loss\n",
    "            mse_loss = nn.MSELoss()\n",
    "            value_loss = mse_loss(values, expected_returns.detach())\n",
    "            \n",
    "            # log results\n",
    "            _episode_reward = sum(rewards)\n",
    "            running_rewards.append(_episode_reward)\n",
    "            running_mean = np.array(running_rewards).mean()\n",
    "            running_std_dev = np.array(running_rewards).std()\n",
    "            if (episode+1) % 100 == 0:\n",
    "                print(f\"Episode {episode+1}\\taverage reward: {running_mean:.2f}, std dev: {running_std_dev:.2f}\")\n",
    "                \n",
    "            if running_mean > 150:\n",
    "                print(f\"Solved! Running reward is now {running_mean:.2f}\")\n",
    "                print(f\"Episode {episode+1}\\taverage reward: {running_mean:.2f}, std dev: {running_std_dev:.2f}\")\n",
    "                break\n",
    "            \n",
    "            # optimize the model with backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # optimize the model with backpropagation\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\taverage reward: -118.09, std dev: 81.62\n",
      "Episode 200\taverage reward: -77.20, std dev: 30.90\n",
      "Episode 300\taverage reward: -55.22, std dev: 84.67\n",
      "Episode 400\taverage reward: -23.79, std dev: 54.12\n",
      "Episode 500\taverage reward: 31.83, std dev: 91.50\n",
      "Episode 600\taverage reward: 56.44, std dev: 119.68\n",
      "Episode 700\taverage reward: 10.43, std dev: 124.32\n",
      "Episode 800\taverage reward: 28.55, std dev: 66.03\n",
      "Episode 900\taverage reward: 50.65, std dev: 78.87\n",
      "Episode 1000\taverage reward: 94.40, std dev: 48.34\n",
      "Episode 1100\taverage reward: 48.90, std dev: 84.25\n",
      "Episode 1200\taverage reward: 76.41, std dev: 62.40\n",
      "Episode 1300\taverage reward: 74.27, std dev: 79.07\n",
      "Episode 1400\taverage reward: 96.90, std dev: 65.42\n",
      "Episode 1500\taverage reward: 75.55, std dev: 89.40\n",
      "Episode 1600\taverage reward: 88.47, std dev: 69.44\n",
      "Solved! Running reward is now 150.61\n",
      "Episode 1679\taverage reward: 150.61, std dev: 62.11\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "reinforce_baseline_agent = ReinforceWithBaseline(env)\n",
    "reinforce_baseline_agent.train(num_episodes=int(3e3), max_steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
