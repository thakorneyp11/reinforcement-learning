{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from algorithms import QLearning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Taxi environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Environment information\n",
    "ref: https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py#L24\n",
    "\n",
    "Actions\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east\n",
    "    - 3: move west\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "Observations\n",
    "    There are 500 discrete states since there are 25 taxi positions, 5 possible\n",
    "    locations of the passenger (including the case when the passenger is in the\n",
    "    taxi), and 4 destination locations.\n",
    "    Note that there are 400 states that can actually be reached during an\n",
    "    episode. The missing states correspond to situations in which the passenger\n",
    "    is at the same location as their destination, as this typically signals the\n",
    "    end of an episode. Four additional states can be observed right after a\n",
    "    successful episodes, when both the passenger and the taxi are at the destination.\n",
    "    This gives a total of 404 reachable discrete states.\n",
    "    Each state space is represented by the tuple:\n",
    "    (taxi_row, taxi_col, passenger_location, destination)\n",
    "    An observation is an integer that encodes the corresponding state.\n",
    "    The state tuple can then be decoded with the \"decode\" method.\n",
    "    Passenger locations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "    Destinations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "Info\n",
    "    ``step`` and ``reset()`` will return an info dictionary that contains \"p\" and \"action_mask\" containing\n",
    "        the probability that the state is taken and a mask of what actions will result in a change of state to speed up training.\n",
    "    As Taxi's initial state is a stochastic, the \"p\" key represents the probability of the\n",
    "    transition however this value is currently bugged being 1.0, this will be fixed soon.\n",
    "    As the steps are deterministic, \"p\" represents the probability of the transition which is always 1.0\n",
    "    For some cases, taking an action will have no effect on the state of the agent.\n",
    "    In v0.25.0, ``info[\"action_mask\"]`` contains a np.ndarray for each of the action specifying\n",
    "    if the action will change the state.\n",
    "    To sample a modifying action, use ``action = env.action_space.sample(info[\"action_mask\"])``\n",
    "    Or with a Q-value based algorithm ``action = np.argmax(q_values[obs, np.where(info[\"action_mask\"] == 1)[0]])``.\n",
    "Rewards\n",
    "    - -1 per step unless other reward is triggered.\n",
    "    - +20 delivering passenger.\n",
    "    - -10  executing \"pickup\" and \"drop-off\" actions illegally.\n",
    "\"\"\"\n",
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 0 1\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "taxi_row, taxi_col, passenger_location, destination = env.decode(state)\n",
    "print(taxi_row, taxi_col, passenger_location, destination)\n",
    "env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTaxi(QLearning):\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        super().__init__(env, alpha, gamma, epsilon)\n",
    "        \n",
    "        # CUSTOMIZE: the environment shape for Taxi-v3 is (5, 5)\n",
    "        self.env_shape = (5, 5)\n",
    "\n",
    "    # TODO: add display option for visualization of the episode\n",
    "    def generate_sample_episode(self, display=False, always_greedy=False):\n",
    "        episode = list()\n",
    "    \n",
    "        state = self.env.reset()\n",
    "        while True:\n",
    "            timestep = list()\n",
    "            prev_state = state\n",
    "            \n",
    "            action = self.choose_action(state, always_greedy=always_greedy)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            timestep.append(prev_state)\n",
    "            timestep.append(action)\n",
    "            timestep.append(reward)\n",
    "            episode.append(timestep)\n",
    "\n",
    "            if done:\n",
    "                # include the terminal state\n",
    "                episode.append([state, None, None])\n",
    "                break\n",
    "        \n",
    "        return episode\n",
    "\n",
    "    # CUSTOMIZE: for Taxi environment\n",
    "    def evaluate_policy(self, episodes, display=False):\n",
    "        wins = 0\n",
    "        for _ in range(episodes):\n",
    "            episode = self.generate_sample_episode(display=False)\n",
    "            if episode[-2][-1] == 20:\n",
    "                wins += 1\n",
    "        return wins / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "qlearning_agent = QLearningTaxi(env)\n",
    "qlearning_agent.train(episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 94.00% from 100 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "test_episodes = 100\n",
    "win_rate = qlearning_agent.evaluate_policy(episodes=test_episodes)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table shape:  (500, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [-13.72768255, -13.74479488, -13.75266806, -13.99915753,\n",
       "        -12.61982832, -15.72226812],\n",
       "       [ -9.16139051,  -8.7947854 ,  -9.18620862,  -9.67581796,\n",
       "         -6.90165854,  -9.23304457],\n",
       "       ...,\n",
       "       [ -9.68684327,  -8.65593796,  -9.29741165,  -9.36345029,\n",
       "         -9.7239857 ,  -9.47741436],\n",
       "       [-15.07525683, -14.39023442, -14.71135515, -15.04737902,\n",
       "        -15.13193599, -15.87478191],\n",
       "       [ -5.3518972 ,  -4.57890024,  -5.26697884,  -2.44609066,\n",
       "         -5.3862768 ,  -4.77487501]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Q-table shape: \", qlearning_agent.Q.shape)\n",
    "qlearning_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[481, 3, -1],\n",
       " [461, 0, -1],\n",
       " [461, 1, -1],\n",
       " [361, 1, -1],\n",
       " [261, 3, -1],\n",
       " [241, 3, -1],\n",
       " [221, 3, -1],\n",
       " [201, 1, -1],\n",
       " [101, 1, -1],\n",
       " [1, 4, -1],\n",
       " [17, 2, -1],\n",
       " [37, 0, -1],\n",
       " [137, 0, -1],\n",
       " [237, 2, -1],\n",
       " [257, 2, -1],\n",
       " [277, 5, -10],\n",
       " [277, 0, -1],\n",
       " [377, 1, -1],\n",
       " [277, 1, -1],\n",
       " [177, 1, -1],\n",
       " [77, 2, -1],\n",
       " [97, 5, 20],\n",
       " [85, None, None]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample episode (state, action, reward)\n",
    "qlearning_agent.generate_sample_episode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearning(QLearningTaxi):\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        super().__init__(env, alpha, gamma, epsilon)\n",
    "\n",
    "        self.Q = None\n",
    "        self.Q1 = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.Q2 = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def choose_action(self, state, always_greedy=False):\n",
    "        if always_greedy:\n",
    "            return np.argmax(self.Q1[state] + self.Q2[state])\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.Q1[state] + self.Q2[state])\n",
    "\n",
    "    def update_Q(self, states, actions, rewards):\n",
    "        \"\"\"\n",
    "        Updates the Q-table using the double Q-learning update rule\n",
    "        \"\"\"\n",
    "        state, action, reward = states[0], actions[0], rewards[0]\n",
    "        next_state = states[-1]\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            a_best = np.argmax(self.Q1[next_state])\n",
    "            self.Q1[state, action] += self.alpha * (reward + self.gamma * self.Q2[next_state, a_best] - self.Q1[state, action])\n",
    "        else:\n",
    "            a_best = np.argmax(self.Q2[next_state])\n",
    "            self.Q2[state, action] += self.alpha * (reward + self.gamma * self.Q1[next_state, a_best] - self.Q2[state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "dqlearning_agent = DoubleQLearning(env)\n",
    "dqlearning_agent.train(episodes=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent win rate: 99.00% from 100 test episodes\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "test_episodes = 100\n",
    "win_rate = dqlearning_agent.evaluate_policy(episodes=test_episodes)\n",
    "print(f'agent win rate: {win_rate*100 :.2f}% from {test_episodes} test episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table 1 shape:  (500, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [-20.75542918, -19.92252507, -19.09488525, -20.33213634,\n",
       "        -13.29708345, -19.84209317],\n",
       "       [-11.50147523, -12.16690577, -12.5123893 , -10.98573195,\n",
       "         -6.66240232, -10.6336982 ],\n",
       "       ...,\n",
       "       [ -8.06850751,  -5.98364111,  -7.77893965,  -8.7499446 ,\n",
       "         -8.96907139,  -7.97492356],\n",
       "       [-16.37555612, -15.46464557, -16.13693798, -15.93362815,\n",
       "        -17.08503729, -15.6740936 ],\n",
       "       [ -3.53572206,  -3.84330201,  -3.25997173,  -2.24443221,\n",
       "         -3.57953828,  -2.77487504]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Q-table 1 shape: \", dqlearning_agent.Q1.shape)\n",
    "dqlearning_agent.Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table 2 shape:  (500, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [-19.54806922, -21.23248661, -21.9285479 , -19.88538524,\n",
       "        -13.03468397, -21.2986652 ],\n",
       "       [-13.22547207, -11.53844687, -12.71131201, -12.18916618,\n",
       "         -7.33613326, -13.13661204],\n",
       "       ...,\n",
       "       [ -8.19410617,  -6.39092793,  -9.25827374,  -7.92731667,\n",
       "         -7.54083315,  -8.34796327],\n",
       "       [-16.32042095, -17.30005405, -17.31658263, -17.21567391,\n",
       "        -16.59577138, -18.08610177],\n",
       "       [ -2.83890115,  -3.25159896,  -3.13839664,  -2.63395011,\n",
       "         -2.98031826,  -3.44827867]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Q-table 2 shape: \", dqlearning_agent.Q2.shape)\n",
    "dqlearning_agent.Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.012514196510188183"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1 and Q2 average value difference\n",
    "np.mean(dqlearning_agent.Q1 - dqlearning_agent.Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[329, 1, -1],\n",
       " [229, 3, -1],\n",
       " [209, 0, -1],\n",
       " [309, 0, -1],\n",
       " [409, 0, -1],\n",
       " [409, 4, -1],\n",
       " [417, 1, -1],\n",
       " [317, 1, -1],\n",
       " [217, 2, -1],\n",
       " [237, 2, -1],\n",
       " [257, 1, -1],\n",
       " [157, 1, -1],\n",
       " [57, 2, -1],\n",
       " [77, 2, -1],\n",
       " [97, 5, 20],\n",
       " [85, None, None]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample episode (state, action, reward)\n",
    "dqlearning_agent.generate_sample_episode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
